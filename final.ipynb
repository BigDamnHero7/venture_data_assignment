{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import previous data\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def download_nyc_taxi_data(data_type, start_date, dest_folder,time_back):\n",
    "    \"\"\"\n",
    "    Downloads NYC TLC trip data for a specific data type from the start date to the past 3 years.\n",
    "\n",
    "    Args:\n",
    "        data_type: \"yellow\" or \"green\" for trip data type.\n",
    "        start_date: The start date in the format \"yyyy-mm\" (e.g., \"2024-01\").\n",
    "    \"\"\"\n",
    "    # Base URL for data download\n",
    "    base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "\n",
    "    # Validate data type\n",
    "    if data_type not in (\"yellow\", \"green\"):\n",
    "        raise ValueError(f\"Invalid data type: {data_type}. Choose 'yellow' or 'green'.\")\n",
    "\n",
    "    # Parse the start date\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m\")\n",
    "\n",
    "    # Calculate the end date (3 years before the start date)\n",
    "    end_date = start_date - relativedelta(months=time_back) #to go back years instead of months change months to years\n",
    "\n",
    "    # Generate the list of year-month strings for each month in the past 3 years\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date > end_date:\n",
    "        date_list.append(current_date.strftime(\"%Y-%m\"))\n",
    "        current_date -= relativedelta(months=1)\n",
    "\n",
    "    for year_month in date_list:\n",
    "        # Construct filename\n",
    "        filename = f\"{data_type}_tripdata_{year_month}.parquet\"\n",
    "        filepath = os.path.join(dest_folder, filename)\n",
    "\n",
    "        # Full download URL\n",
    "        download_url = os.path.join(base_url, filename)\n",
    "\n",
    "        # Download data\n",
    "        response = requests.get(download_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Save data directly to the current directory\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {data_type} trip data for {year_month} to {filepath}\")\n",
    "        else:\n",
    "            print(f\"Error downloading data for {year_month}: {response.status_code}\")\n",
    "\n",
    "# Example usage\n",
    "#data_type = \"yellow\"\n",
    "#start_date = \"2024-01\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import latest data\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def download_nyc_taxi_data(data_type):\n",
    "    \"\"\"\n",
    "    Downloads NYC TLC trip data for a specific data type for four months prior to the current date.\n",
    "\n",
    "    Args:\n",
    "        data_type: \"yellow\" or \"green\" for trip data type.\n",
    "    \"\"\"\n",
    "    # Base URL for data download\n",
    "    base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "\n",
    "    # Validate data type\n",
    "    if data_type not in (\"yellow\", \"green\"):\n",
    "        raise ValueError(f\"Invalid data type: {data_type}. Choose 'yellow' or 'green'.\")\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    # Calculate the year and month for four months ago\n",
    "    four_months_ago = current_date - timedelta(days=30*4)\n",
    "    year_month = four_months_ago.strftime(\"%Y-%m\")\n",
    "\n",
    "    # Construct filename\n",
    "    filename = f\"{data_type}_tripdata_{year_month}.parquet\"\n",
    "\n",
    "    # Full download URL\n",
    "    download_url = os.path.join(base_url, filename)\n",
    "\n",
    "    # Download data\n",
    "    response = requests.get(download_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Save data directly to the current directory\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {data_type} trip data for {year_month} to {filename}\")\n",
    "    else:\n",
    "        print(f\"Error downloading data: {response.status_code}\")\n",
    "\n",
    "# Example usage\n",
    "data_type = \"yellow\"\n",
    "download_nyc_taxi_data(data_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "def get_output_schema():\n",
    "    schema = pa.schema([\n",
    "        ('VendorID', pa.int64()),\n",
    "        ('tpep_pickup_datetime', pa.timestamp('s')),\n",
    "        ('tpep_dropoff_datetime', pa.timestamp('s')),\n",
    "        ('passenger_count', pa.int64()),\n",
    "        ('trip_distance', pa.float64()),\n",
    "        ('RatecodeID', pa.int64()),\n",
    "        ('store_and_fwd_flag', pa.string()),\n",
    "        ('PULocationID', pa.int64()),\n",
    "        ('DOLocationID', pa.int64()),\n",
    "        ('payment_type', pa.int64()),\n",
    "        ('fare_amount', pa.float64()),\n",
    "        ('extra', pa.float64()),\n",
    "        ('mta_tax', pa.float64()),\n",
    "        ('tip_amount', pa.float64()),\n",
    "        ('tolls_amount', pa.float64()),\n",
    "        ('improvement_surcharge', pa.float64()),\n",
    "        ('total_amount', pa.float64()),\n",
    "        ('congestion_surcharge', pa.float64()),\n",
    "        ('tpep_pickup_hour', pa.int64()),\n",
    "        ('tpep_dropoff_hour', pa.int64()),\n",
    "        ('tpep_pickup_dayofweek', pa.int64()),\n",
    "        ('tpep_dropoff_dayofweek', pa.int64())\n",
    "    ])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def calculate_avg_distance_per_hour(df):\n",
    "    # Converting to datetime type\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "    # Extract hour component\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
    "\n",
    "    # Avg distance by pickup hour\n",
    "    avg_dist_pickup_hour = df.groupby('pickup_hour')['trip_distance'].mean()\n",
    "\n",
    "    # Avg distance by dropoff hour\n",
    "    avg_dist_dropoff_hour = df.groupby('dropoff_hour')['trip_distance'].mean()\n",
    "\n",
    "    # Combine results\n",
    "    avg_dist_per_hour = pd.concat([avg_dist_pickup_hour, avg_dist_dropoff_hour], axis=0)\n",
    "    avg_dist_per_hour = avg_dist_per_hour.groupby(level=0).mean()\n",
    "\n",
    "    return avg_dist_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_busiest_hours(df, top_n=3):\n",
    "    \n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    \n",
    "    #count number of trips for each hour\n",
    "    trips_per_hour = df['pickup_hour'].value_counts().sort_index()\n",
    "    \n",
    "    #find the busiest hours\n",
    "    top_busiest_hours = trips_per_hour.nlargest(top_n)\n",
    "    \n",
    "    return top_busiest_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hour_week_level(df):\n",
    "    #Convert datetime columns to datetime type\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "    #Extract hour features\n",
    "    df['tpep_pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    df['tpep_dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
    "\n",
    "    #Extract day of week (1=Monday, 2=Tuesday, ..., 7=Sunday)\n",
    "    df['tpep_pickup_dayofweek'] = df['tpep_pickup_datetime'].dt.dayofweek + 1\n",
    "    df['tpep_dropoff_dayofweek'] = df['tpep_dropoff_datetime'].dt.dayofweek + 1\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro\n",
    "from fastavro.schema import load_schema\n",
    "from fastavro import writer as avro_writer\n",
    "def save_as_avro(df, file_path, schema_file):\n",
    "    records = df.to_dict(orient='records')\n",
    "    schema = load_schema(schema_file)\n",
    "    with open(file_path, 'wb') as out:\n",
    "        avro_writer(out, schema, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ./data\\yellow_tripdata_2023-06.parquet\n",
      "processing ./data\\yellow_tripdata_2023-07.parquet\n",
      "processing ./data\\yellow_tripdata_2023-08.parquet\n",
      "processing ./data\\yellow_tripdata_2023-09.parquet\n",
      "processing ./data\\yellow_tripdata_2023-10.parquet\n",
      "processing ./data\\yellow_tripdata_2023-11.parquet\n",
      "processing ./data\\yellow_tripdata_2023-12.parquet\n",
      "processing ./data\\yellow_tripdata_2024-01.parquet\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(with_features\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#find top busiest hours\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''busiest_hours = top_busiest_hours(combined_dfs)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mprint(\"Top busiest hours (combined data):\\n\",busiest_hours)'''\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m with_features \u001b[38;5;241m=\u001b[39m \u001b[43mhour_week_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_dfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 10 rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(with_features\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m, in \u001b[0;36mhour_week_level\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhour_week_level\u001b[39m(df):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#Convert datetime columns to datetime type\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtpep_pickup_datetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Extract hour features\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_type = \"yellow\"\n",
    "    start_date = \"2024-01\"\n",
    "    dest_folder = \"./data\"\n",
    "    time_back = 5\n",
    "    \n",
    "    # Check if data is already downloaded\n",
    "    all_files_exist = True\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m\")\n",
    "    end_date = current_date - relativedelta(months=time_back) #to go back years instead of months change months to years\n",
    "\n",
    "    while current_date > end_date:\n",
    "        filename = f\"{data_type}_tripdata_{current_date.strftime('%Y-%m')}.parquet\"\n",
    "        file_path = os.path.join(dest_folder, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            latest_file_date = current_date + relativedelta(months=1) \n",
    "            new_date_to_start = latest_file_date - relativedelta(months=1)\n",
    "            new_date_to_start = new_date_to_start.strftime(\"%Y-%m\")\n",
    "            start_date = new_date_to_start\n",
    "            all_files_exist = False\n",
    "            break\n",
    "        current_date -= relativedelta(months=1)\n",
    "    \n",
    "    \n",
    "    if not all_files_exist:\n",
    "        #download the data\n",
    "        download_nyc_taxi_data(data_type, start_date, dest_folder,time_back)\n",
    "        \n",
    "    all_dfs = []\n",
    "    \n",
    "    #process each downloaded file and append to list\n",
    "    for file_name in os.listdir(dest_folder):\n",
    "        if file_name.endswith('.parquet'):\n",
    "            file_path = os.path.join(dest_folder, file_name)\n",
    "            print(f\"processing {file_path}\")\n",
    "            \n",
    "            #load the file into dfs\n",
    "            df=pd.read_parquet(file_path)\n",
    "            all_dfs.append(df)\n",
    "    \n",
    "    #concat \n",
    "    combined_dfs = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    #calc avg distance per hour\n",
    "    '''avg_distance_per_hour = calculate_avg_distance_per_hour(combined_dfs)\n",
    "    print(\"Average distance per hour (combined data):\\n\",avg_distance_per_hour.head(10))'''\n",
    "    \n",
    "    #find top busiest hours\n",
    "    '''busiest_hours = top_busiest_hours(combined_dfs)\n",
    "    print(\"Top busiest hours (combined data):\\n\",busiest_hours)'''\n",
    "    \n",
    "    with_features = hour_week_level(combined_dfs)\n",
    "    print(\"First 10 rows\")\n",
    "    print(with_features.head(10))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
